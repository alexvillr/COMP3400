\documentclass{article}
\usepackage{mathtools}
\usepackage{minted}
\usepackage{txfonts}
\usepackage{xcolor} % to access the named colour LightGray
\usepackage{geometry}
\definecolor{LightGray}{gray}{0.9}

\geometry{
a4paper,
left=20mm,
right=20mm,
top=20mm,
bottom=20mm
}

\begin{document}
\section{Question 1}

First Notice, 
$$\begin{aligned}
S_n^2 & = \Big( \frac{1}{n} \sum_{k=1}^n{x^2_k}\Big) - \overline{x_n}^2\\
    n\ S_n^2 & = \sum_{k=1}^n{x_k^2} - n\ \overline{x_n}^2\\
    \sum_{k=1}^n{x_k^2} & = n\ S_n^2 + n\ \overline{x_n}^2
\end{aligned}$$

So we can then see,
$$\begin{aligned}
S_{n+1}^2 & = \Big( \frac{1}{n + 1} \sum_{k=1}^{n+1}{x_k^2} \Big) - \overline{x_{n+1}}^2\\
    & = \frac{1}{n + 1} \Big[ \sum_{k=1}^n{x_k^2} + x_{n+1}^2\Big] - \overline{x_{n+1}}^2\\
    & = \frac{1}{n + 1} \Big[ n\ S_n^2 + n\ \overline{x_n}^2 + x_{n+1}^2\Big] - \overline{x_{n+1}}^2\\
    & = \frac{n\ S_n^2 + n\ \overline{x_n}^2 + x_{n+1}^2}{n + 1} - \overline{x_{n+1}}^2\\
    & = \frac{n\ \Big(S_n^2 + \overline{x_n}^2\Big) + x_{n+1}^2}{n + 1} - \overline{x_{n+1}}^2\\
\end{aligned}$$

Hence an equation for $S_{n+1}^2$ using only $(n, S^2_n,  \bar{x}_n, \bar{x}_{n+1}, x_{n+1})$

\clearpage

\section{Question 2}

\begin{minted}
[
frame=lines,
framesep=2mm,
baselinestretch=1.2,
bgcolor=LightGray,
fontsize=\footnotesize,
linenos
]
{haskell}
lcMean :: [Float] -> Float
lcMean [] = undefined
lcMean [x] = x
lcMean (x:xs) = (x + n * average) / (n + 1)
  where
    average = lcMean xs
    n       = fromIntegral (length xs)

lcVariance :: [Float] -> Float
lcVariance []     = undefined
lcVariance [x]    = 0
lcVariance (x:xs) = (((n * s') + (n * (avg ** 2)) + (x ** 2)) / (n + 1)) - (avg' ** 2)
  where
    avg = lcMean xs
    avg' = lcMean (x:xs)
    s' = lcVariance xs
    n = fromIntegral (length xs)
\end{minted}

\section{Question 3}

\begin{minted}
[
frame=lines,
framesep=2mm,
baselinestretch=1.2,
bgcolor=LightGray,
fontsize=\footnotesize,
linenos
]
{haskell}
trMean :: [Float] -> Float
trMean [] = undefined
trMean xs = htrMean 0 0 xs

htrMean :: Float -> Float -> [Float] -> Float
htrMean _ avg []     = avg
htrMean n avg (x:xs) = htrMean n' avg' xs
  where
    n'   = n + 1
    avg' = (x + (avg * n)) / (n + 1)

trVariance :: Float -> Float -> Float -> [Float] -> Float
trVariance _ _ s []      = s
trVariance n avg s (x:xs) = trVariance n' avg' s' xs
  where
    n'   = n + 1
    avg' = htrMean n avg [x]
    s'   = ((n * (s + (avg ** 2)) + (x ** 2)) / (n + 1)) - (avg' ** 2)
\end{minted}

\section{Question 4}

\begin{minted}
[
frame=lines,
framesep=2mm,
baselinestretch=1.2,
bgcolor=LightGray,
fontsize=\footnotesize,
linenos
]
{haskell}
variance :: [Float] -> Float
variance [] = undefined
variance xs = trVariance 0 0 0 xs
\end{minted}

\clearpage

\section{Question 5}

\textbf{Iteration invariant:}

where ys is everything processed already and $n = \text{length } ys$
$$
\text{trVariance } n \ \overline{x} \ S^2_n \ (x:xs) = (n + 1) \ \overline{x:ys} \ S^2_{n + 1} \ xs
$$

\section{Question 6}

\textbf{Iteration invariant proof:}

$$\begin{aligned}
\text{trVariance } n \ \overline{x} \ S^2_n \ \emptyset & = s\\
& = \text{variance} \ ys
\end{aligned}$$

Meaning that we have the variance of the whole list that has now been processed.

---

$$
\begin{aligned}
        \text{trVariance } n \ \overline{x} \ S^2_n \ (x:xs) & = \text{trVariance } (n + 1) \ (\overline{x:ys}) \ \Big(\frac{n \times (S^2_n + \overline{x}^2) + x^2}{n + 1} - \overline{x:ys}^2\Big) \ xs\\
        & = \text{trVariance } (n + 1) \ \overline{x:ys} \ \Big(\frac{\frac{n \times \sum_{k=1}^n ys_k^2}{n} + (x^2)}{n + 1} - \overline{x:ys}^2\Big) \ xs\\
        & = \text{trVariance } (n + 1) \ \overline{x:ys} \frac{\sum_{k=1}^n ys_k^2 + x^2}{n + 1} - \overline{x:ys}^2\Big) \ xs\\
        & = \text{trVariance } (n + 1) \ \overline{x:ys} \ \Big(\frac{\sum^{n+1}_{k=1}(x:ys)_k^2}{n + 1} - \overline{x:ys}^2 \Big) \ xs\\
        & = \text{trVariance } (n + 1) \ \overline{x:ys} \ \Bigg(\Big(\frac{1}{n + 1} \times \sum_{k=1}^{n+1} (x:ys)_k^2\Big) - \overline{x:ys}^2 \Bigg) \ xs\\
        & = \text{trVariance } (n + 1) \ \overline{x:ys} \ S^2_{n+1} \ xs
\end{aligned}
$$

Meaning that we have the variance of (ys:x) and will continue to process with the rest of the list that hasn't been processed yet.

---

$$
\begin{aligned}
        \text{trVariance } 0\ 0\ 0\ (x:xs) & = \text{trVariance } (0 + 1)\ \overline{x}\ \Big(\frac{0 \times (0 + 0^2) + x^2)}{1} - \overline{x}^2\Big)\ xs \\
        & = \text{trVariance } 1\ \overline{x}\ \Big(\frac{x^2}{1} - \overline{x}^2\Big)\ xs\\
        & = \text{trVariance } 1\ x\ 0\ xs\\
        & = \text{trVariance } 1\ \overline{x}\ (\text{variance } x)\ xs
\end{aligned}
$$

Meaning that the first call will always return a variance of 0 which is correct for the variance of 1 value.

And thus we have our formula for variance to satisfy our iteration invariant for a base case, a recursive case, and our final (edge) case

\clearpage

\section{Question 7}

\textbf{Bound Value:}

Our bound value would be the length of our input list xs

\section{Question 8}

\textbf{Decreasing and positivity of bound value:}

Each time we take an element out of xs and process it so each iteration shortens the length of the list. The length will always be positive because our function can never be called on an empty list, and once we run out of elements in the list we return and finish. Thus we have a value which is always positive and decreases every iteration down to zero.

\section{Question 9}

\textbf{Quick Checks:}

\begin{minted}
[
frame=lines,
framesep=2mm,
baselinestretch=1.2,
bgcolor=LightGray,
fontsize=\footnotesize,
linenos
]
{haskell}
-- Var(X) > 0
property1 :: IO()
property1 = quickCheck $ \xs -> null xs || variance xs >= 0

-- Var(X + c) = Var(X), where c is some constant.
property2 :: IO()
property2 = quickCheck $ \a xs -> null xs || diff < 10**(-2)
  where
    diff    = abs (variance xsplusa - variance xs)
    xsplusa = map (+ a) xs

-- Var(aX) = a^2 * Var(X), where a is some constant.
property3 :: IO()
property3 = quickCheck $ \a xs -> null xs || diff < 10**(-2)
  where
    diff             = abs (atimesxsvar - asquaredtimesvar)
    atimesxsvar      = variance (map (a *) xs)
    asquaredtimesvar = (a ** 2) * variance xs

-- Var(aX + b) = a^2 * Var(X), the result of the previous two properties combined.
property4 :: IO()
property4 = quickCheck $ \a b xs -> null xs || diff < 10**(-2)
  where
    diff             = abs (atimesxsplusbvar - asquaredtimesvar)
    atimesxsplusbvar = variance map (\x -> a * x + b) xs
    asquaredtimesvar = (a ** 2) * variance xs
\end{minted}

These are based on the properties of population variance.

The first property of variance is that variance will always be a positive number.

The second property of variance is that if you add some constant to every value in the list, the variance does not change.

The third property of variance is that if you multiply some constant with every value in the list, the variance is the same as if you didn't do that multiplication and then multiplied the result by the constant squared.

The final property is that the second and third property can be combined.

\clearpage

\section{Question 10}

$$
\begin{aligned}
\text{Given that, Plus } m \text{ Zero } & = m \text{ Using PMI, prove that,}\\
\text{Plus Zero } n & = n 
\end{aligned}
$$

\subsection{\textbf{Base Case}, $n = \text{Zero}$}

    $$
    \text{Plus Zero n } \implies \text{Plus Zero Zero}
    $$
    $$
    \text{Plus Zero Zero } = \text{ Zero}
    $$

    This is true as $0 + 0 = 0$ under the naturals.

\subsection{\textbf{Inductive Hypothesis}, $n = k$}

    Assume that $\text{Plus Zero } k = k$
        
\subsection{\textbf{Inductive Step}}

    for our $k + 1$ case, we are simply using $\text{Succ } k$, to get

    $$
    \text{Plus Zero } ( \text{Succ } k ) = \text{Succ } ( \text{Plus Zero } k )
    $$

    By definition, and since we have assumed in our inductive hypothesis that $\text{Plus Zero } k = k$ we have
    
    $$
    \text{Plus Zero } ( \text{Succ } k) = \text{Succ } k
    $$

    Or in other words,

    $$
    0 + (k + 1) = k + 1
    $$

    which is true under the natural numbers as well. Thus because of our base case being correct, and our inductive step being correct if we assume that our equation works for some kth step. By the Principle of Mathematical Induction, we know that $\text{Plus Zero n} = n,\ \forall\ n \in \mathbb{N}$

\clearpage

\section{Question 11}

Using PMI Prove the commutativity of Plus, that is,
$$
\text{Using PMI prove that, Plus}\ m\ n\ = \text{Plus}\ n\ m
$$

\subsection{\textbf{Base Case}, $n = \text{Zero}$}

    $$
    \begin{aligned}
        \text{LHS } & = \text{ Plus m Zero} = m, \text{ By construction}\\
        \text{RHS } & = \text{ Plus Zero m} = m, \text{ From our conclusions in Q10}\\
        \text{LHS } & = \text{ RHS}
    \end{aligned}
    $$
    

\subsection{\textbf{Inductive Hypothesis}, $n = k$}

    Assume that,
    $$
    \text{Plus}\ m\ k = \text{Plus}\ k\ m
    $$

\subsection{\textbf{Inductive Step}}

    For our $k + 1$ case, we are using $\text{Succ}\ k$ to represent $k + 1$, if we then let 
    \begin{equation}
        z = \text{Plus}\ m\ k = \text{Plus}\ k\ m\ (\text{From our inductive hypothesis.})
    \end{equation}
    This gives us
    
    $$
    \begin{aligned}
        \text{LHS } & = \text{Plus}\ m\ (\text{Succ}\ k)\\
            & = \text{Succ}\ (\text{Plus}\ m\ k),\ \text{By definition}\\
            & = \text{Succ}\ z, \text{From (1)}\\
        \text{RHS } & = \text{Plus}\ (\text{Succ}\ k)\ m\\
            & = \text{Plus}\ k\ (\text{Succ}\ m),\ \text{Assuming Plus is associative}\\
            & = \text{Succ}\ (\text{Plus}\ k\ m)\\
            & = \text{Succ}\ z, \text{From (1)}\\
        \text{LHS } & = \text{ RHS, By our Inductive hypothesis}
    \end{aligned}
    $$

    Since our LHS is equal to our RHS under our assumption in our inductive hypothesis. Our base case has been proven true. By the Principle of Mathematical Induction, we have proven that Plus is commutative when we iterate over n. We can see that the converse is also true with a symmetric proof. 

\end{document}